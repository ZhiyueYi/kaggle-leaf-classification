{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Classfication \n",
    "A For CZ4041 Machine Learning Assignment from PT3 in AY2018/2019 Semester 2.\n",
    "The group members are:\n",
    "- LIU Yiqing\n",
    "- LUO Bingyi\n",
    "- TENG He Xu\n",
    "- WANG Jia\n",
    "- YI Zhiyue\n",
    "- ZHAO Ziru\n",
    "\n",
    "The Kaggle problem is here https://www.kaggle.com/c/leaf-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import History \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histories are to record model losses in every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = History()\n",
    "history2 = History()\n",
    "history3 = History()\n",
    "\n",
    "LABEL_PATH = 'data/'\n",
    "TRAIN_FILE_NAME = 'train.csv'\n",
    "TEST_FILE_NAME = 'test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load features and labels from the train csv file\n",
    "\n",
    "y is extracted from the `species` column and converted from text string to numeric values as classes\n",
    "\n",
    "x contains features in the remaining columns except `id` columns. `StandardScaler` is used to transform the data so that its distribution will have a `mean = 0` and standard `deviation = 1`. It is to standardize the scale of the data for ease of computation and remain the features unaffected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_frame = pd.read_csv(LABEL_PATH + TRAIN_FILE_NAME)\n",
    "\n",
    "train_data_frame = train_data_frame.drop(['id'], axis=1)\n",
    "\n",
    "y = train_data_frame.pop('species')\n",
    "classes = np.unique(y)\n",
    "\n",
    "y = to_categorical(LabelEncoder().fit(y).transform(y))\n",
    "\n",
    "x = StandardScaler().fit(train_data_frame).transform(train_data_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `StratifiedShuffleSplit` to randomly split the data set into training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x dimention:  (792, 192)\n",
      "validate_x dimention:    (198, 192)\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=0.2,random_state=12345)\n",
    "\n",
    "train_index, validation_index = next(iter(sss.split(x, y)))\n",
    "train_x, validate_x = x[train_index], x[validation_index]\n",
    "train_y, validate_y = y[train_index], y[validation_index]\n",
    "print(\"train_x dimention: \",train_x.shape)\n",
    "print(\"validate_x dimention:   \",validate_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the number of classes for later computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_classes = len(np.unique(train_y, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "\n",
    "Use ensemble learning method to predict the value.\n",
    "\n",
    "Ensemble learning refers to training multiple models with the same set of training data set and validation data set. With multiple sets of models, a pool of predicted values can be generated. We can pick the most possible predicted value from the pool to achieve the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Dense(250, activation='relu', input_dim = train_x.shape[1]))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(150, activation='relu'))\n",
    "model1.add(Dropout(0.4))\n",
    "model1.add(Dense(no_of_classes, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(1000, activation='tanh', input_dim = train_x.shape[1]))\n",
    "model2.add(Dense(1000, activation='relu'))\n",
    "model2.add(Dense(1000, activation='relu'))\n",
    "model2.add(Dense(no_of_classes, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(500, activation='relu', input_dim = train_x.shape[1]))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(500, activation='relu'))\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(no_of_classes, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 250)               48250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 150)               37650     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 99)                14949     \n",
      "=================================================================\n",
      "Total params: 100,849\n",
      "Trainable params: 100,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              193000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1000)              1001000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 99)                99099     \n",
      "=================================================================\n",
      "Total params: 2,294,099\n",
      "Trainable params: 2,294,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 500)               96500     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 99)                49599     \n",
      "=================================================================\n",
      "Total params: 396,599\n",
      "Trainable params: 396,599\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and fit the model\n",
    "\n",
    "At this stage, the data is pumped into the model and Keras will help to run iterations to reduce the loss as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x171c05a8240>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model1.fit(train_x, train_y, epochs = 50, verbose=0, validation_data=(validate_x, validate_y), callbacks=[history1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x171c0e12438>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model2.fit(train_x, train_y, epochs = 10, verbose=0, validation_data=(validate_x, validate_y), callbacks=[history2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x171c1450908>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(train_x, train_y, epochs = 20, verbose=0, validation_data=(validate_x, validate_y), callbacks=[history3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models\n",
    "\n",
    "Trained model information is saved for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('models/model_1_0.29073.h5')\n",
    "model2.save('models/model_2_0.29073.h5')\n",
    "model3.save('models/model_3_0.29073.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same data pre-processing procedures on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_frame = pd.read_csv(LABEL_PATH + TEST_FILE_NAME)\n",
    "\n",
    "index = test_data_frame.pop('id')\n",
    "\n",
    "test_x = StandardScaler().fit(test_data_frame).transform(test_data_frame)\n",
    "#test_x = test_data_frame.get_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separately use the 3 models to predict the results based on the test X values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_1 = model1.predict_classes(test_x)\n",
    "test_y_2 = model2.predict_classes(test_x)\n",
    "test_y_3 = model3.predict_classes(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summarised `test_y` object is generated and sorted by `loss` in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = [\n",
    "    {\n",
    "        'name': 'test_y_1',\n",
    "        'predict': test_y_1,\n",
    "        'loss': history1.history['loss'][-1]\n",
    "    }, {\n",
    "        'name': 'test_y_2',\n",
    "        'predict': test_y_2,\n",
    "        'loss': history2.history['loss'][-1]\n",
    "    }, {\n",
    "        'name': 'test_y_3',\n",
    "        'predict': test_y_3,\n",
    "        'loss': history3.history['loss'][-1]\n",
    "    }, \n",
    "]\n",
    "\n",
    "test_y = sorted(test_y, key=lambda k: k['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data_grid` is used to store the generated the file format which Kaggle competition will accept.\n",
    "\n",
    "Then iterate every predicted data and compare the results in the 3 models. \n",
    "\n",
    "If 2 or more models predict the same class, it will be the actual predicted class.\n",
    "\n",
    "If 3 models all predict different classes, then the value of the model with the least loss will be athe actual predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_grid = np.zeros((len(test_y_1), len(classes)))\n",
    "\n",
    "for i in range(len(test_y_1)):\n",
    "    count = {}\n",
    "    for test in test_y:\n",
    "        if test['predict'][i] not in count:\n",
    "            count[test['predict'][i]] = 1\n",
    "        else:\n",
    "            count[test['predict'][i]] += 1\n",
    "    \n",
    "    result = Counter(count)\n",
    "    predicted = result.most_common(1)\n",
    "    data_grid[i][predicted[0][0]] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pd.DataFrame` to generate CSV format variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pd.DataFrame(data_grid, index = index, columns = classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, write the variable into the CSV file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.csv','w') as file:\n",
    "    file.write(prediction.to_csv())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the same directory, run `kaggle competitions submit -c leaf-classification -f submission.csv -m \"Message\"` command to submit the CSV file to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
